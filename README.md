# Language-Detection-using-BERT
This project uses a pre-trained BERT model for NLP text classification. The model is fine-tuned on labeled data to understand contextual meaning using a transformer-based architecture. The project focuses on practical implementation, training, evaluation, and basic model interpretability.
